{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from PIL import Image\n",
    "\n",
    "MUSHROOMS_PATH = 'mushrooms_dataset'\n",
    "\n",
    "# Directory for the images and its subdirectories\n",
    "images_dir = os.path.join(MUSHROOMS_PATH, 'images')\n",
    "subdirs = [os.path.join(images_dir, subdir) for subdir in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, subdir))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have some ideas for dividing the dataset into training and testing sets. We can use the train_test_split function from scikit-learn to divide the dataset into training and testing sets.\n",
    "#But for that we will have to put the every image into array and then into a dataframe\n",
    "#Then we will have to use ImageDataGenerator and flow_from_dataframe to load the images from the dataframe\n",
    "\n",
    "#Second idea is to manually create the test set by taking 20% of the images from each class and putting them into a separate directory.\n",
    "#We will then use ImageDataGenerator and flow_from_directory to load the images from the directory.\n",
    "\n",
    "#In both ideas we need to take in consider stratification, so that the distribution of classes in the training and testing sets is similar.\n",
    "#For example, if in one class there are 10 images and in another one there are 8 images, we want both  of them to have the same percentage of images in the training and testing sets.\n",
    "\n",
    "#Third idea is to use the splitfolders library to divide the dataset into training and testing sets.\n",
    "#But again we have to stratify the dataset which is not supported by that library.\n",
    "\n",
    "#So the first idea might require a lot of memory usage, the second idea needs us to well do this manually which is not very efficient.\n",
    "#And the third idea is not supporting stratification.\n",
    "\n",
    "#So for now we will use the first idea and divide the dataset into training and testing sets using the train_test_split function from scikit-learn which has the stratify parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So the process with the first idea is as follows:\n",
    "#1. Load the images and its corresponding labels into a dataframe.\n",
    "#2. Divide the dataset into training and testing sets using the train_test_split function from scikit-learn with stratification.\n",
    "#3. Use ImageDataGenerator and flow_from_dataframe to load the images from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for subdir in subdirs:\n",
    "    label = os.path.basename(subdir) #we specify the label for each image\n",
    "    for filename in os.listdir(subdir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            data.append((os.path.join(subdir, filename), label)) #we need to include whole path of the image for using flow_from_dataframe because it reads the images directly from the file system using the paths provided in the DataFrame.\n",
    "data_df = pd.DataFrame(data, columns=['filename', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename                label\n",
       "0  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis\n",
       "1  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis\n",
       "2  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis\n",
       "3  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis\n",
       "4  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_df, test_size=0.2, stratify=data_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 115491 validated image filenames belonging to 1727 classes.\n",
      "Found 38497 validated image filenames belonging to 1727 classes.\n",
      "Found 38498 validated image filenames belonging to 1727 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.2, shear_range=0.2, validation_split=0.25,) #we use 25% from the 80% of the training set as the validation set which will be the same amount as the testing set\n",
    "\n",
    "train_data = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(128, 128),\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "\n",
    "val_data = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(128, 128),\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_data = datagen_test.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(128, 128),\n",
    "    class_mode='categorical',\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---DONE---\n",
    "#It worked, but why do we have only 6903 classes in the test set and 7504 in training and validation sets? \n",
    "# Perhaps there are not enough images in some classes???\n",
    "\n",
    "# It is probably true beacause when we use stratify parameter in train_test_split function, it tries to keep the distribution of classes in the training and testing sets similar.\n",
    "# But if there are not enough images in some classes, it will not be able to keep the distribution of classes similar in the training and testing sets.\n",
    "# So we have few solutions to this\n",
    "# 1. Ensure that each class has a minimum number of instances before splitting the data into training and testing sets - that worked!!!!\n",
    "# 2. Use the stratify sampling only on the classes with sufficient instances, and randomly split the ones with too few instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could have - we could create our own model but since we have massive amount of images its easier to use pre-trained one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the first model\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(7504, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_data, validation_data=val_data, epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will use couple of models to compare each ones results, its good to create a function for saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, model_name):\n",
    "\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    model.save(f'models/{model_name}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on ResNet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1805/1805 [==============================] - 868s 480ms/step - loss: 7.0766 - accuracy: 0.0139 - val_loss: 7.0082 - val_accuracy: 0.0140\n",
      "Epoch 2/10\n",
      "1805/1805 [==============================] - 755s 418ms/step - loss: 7.0092 - accuracy: 0.0143 - val_loss: 6.9702 - val_accuracy: 0.0142\n",
      "Epoch 3/10\n",
      "1805/1805 [==============================] - 759s 421ms/step - loss: 6.9797 - accuracy: 0.0145 - val_loss: 6.9394 - val_accuracy: 0.0147\n",
      "Epoch 4/10\n",
      "1805/1805 [==============================] - 756s 419ms/step - loss: 6.9510 - accuracy: 0.0149 - val_loss: 6.9109 - val_accuracy: 0.0151\n",
      "Epoch 5/10\n",
      "1805/1805 [==============================] - 769s 426ms/step - loss: 6.9203 - accuracy: 0.0152 - val_loss: 6.8801 - val_accuracy: 0.0161\n",
      "Epoch 6/10\n",
      "1805/1805 [==============================] - 773s 428ms/step - loss: 6.8866 - accuracy: 0.0163 - val_loss: 6.8447 - val_accuracy: 0.0167\n",
      "Epoch 7/10\n",
      "1805/1805 [==============================] - 746s 413ms/step - loss: 6.8588 - accuracy: 0.0162 - val_loss: 6.8239 - val_accuracy: 0.0168\n",
      "Epoch 8/10\n",
      "1805/1805 [==============================] - 744s 412ms/step - loss: 6.8335 - accuracy: 0.0167 - val_loss: 6.7984 - val_accuracy: 0.0173\n",
      "Epoch 9/10\n",
      "1805/1805 [==============================] - 744s 412ms/step - loss: 6.8123 - accuracy: 0.0167 - val_loss: 6.7767 - val_accuracy: 0.0174\n",
      "Epoch 10/10\n",
      "1805/1805 [==============================] - 745s 412ms/step - loss: 6.7940 - accuracy: 0.0169 - val_loss: 6.7565 - val_accuracy: 0.0176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27d38387d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(1727, activation='softmax')(x)\n",
    "\n",
    "resnet_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_model.fit(train_data, validation_data=val_data, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv1_pad\n",
      "2 conv1_conv\n",
      "3 conv1_bn\n",
      "4 conv1_relu\n",
      "5 pool1_pad\n",
      "6 pool1_pool\n",
      "7 conv2_block1_1_conv\n",
      "8 conv2_block1_1_bn\n",
      "9 conv2_block1_1_relu\n",
      "10 conv2_block1_2_conv\n",
      "11 conv2_block1_2_bn\n",
      "12 conv2_block1_2_relu\n",
      "13 conv2_block1_0_conv\n",
      "14 conv2_block1_3_conv\n",
      "15 conv2_block1_0_bn\n",
      "16 conv2_block1_3_bn\n",
      "17 conv2_block1_add\n",
      "18 conv2_block1_out\n",
      "19 conv2_block2_1_conv\n",
      "20 conv2_block2_1_bn\n",
      "21 conv2_block2_1_relu\n",
      "22 conv2_block2_2_conv\n",
      "23 conv2_block2_2_bn\n",
      "24 conv2_block2_2_relu\n",
      "25 conv2_block2_3_conv\n",
      "26 conv2_block2_3_bn\n",
      "27 conv2_block2_add\n",
      "28 conv2_block2_out\n",
      "29 conv2_block3_1_conv\n",
      "30 conv2_block3_1_bn\n",
      "31 conv2_block3_1_relu\n",
      "32 conv2_block3_2_conv\n",
      "33 conv2_block3_2_bn\n",
      "34 conv2_block3_2_relu\n",
      "35 conv2_block3_3_conv\n",
      "36 conv2_block3_3_bn\n",
      "37 conv2_block3_add\n",
      "38 conv2_block3_out\n",
      "39 conv3_block1_1_conv\n",
      "40 conv3_block1_1_bn\n",
      "41 conv3_block1_1_relu\n",
      "42 conv3_block1_2_conv\n",
      "43 conv3_block1_2_bn\n",
      "44 conv3_block1_2_relu\n",
      "45 conv3_block1_0_conv\n",
      "46 conv3_block1_3_conv\n",
      "47 conv3_block1_0_bn\n",
      "48 conv3_block1_3_bn\n",
      "49 conv3_block1_add\n",
      "50 conv3_block1_out\n",
      "51 conv3_block2_1_conv\n",
      "52 conv3_block2_1_bn\n",
      "53 conv3_block2_1_relu\n",
      "54 conv3_block2_2_conv\n",
      "55 conv3_block2_2_bn\n",
      "56 conv3_block2_2_relu\n",
      "57 conv3_block2_3_conv\n",
      "58 conv3_block2_3_bn\n",
      "59 conv3_block2_add\n",
      "60 conv3_block2_out\n",
      "61 conv3_block3_1_conv\n",
      "62 conv3_block3_1_bn\n",
      "63 conv3_block3_1_relu\n",
      "64 conv3_block3_2_conv\n",
      "65 conv3_block3_2_bn\n",
      "66 conv3_block3_2_relu\n",
      "67 conv3_block3_3_conv\n",
      "68 conv3_block3_3_bn\n",
      "69 conv3_block3_add\n",
      "70 conv3_block3_out\n",
      "71 conv3_block4_1_conv\n",
      "72 conv3_block4_1_bn\n",
      "73 conv3_block4_1_relu\n",
      "74 conv3_block4_2_conv\n",
      "75 conv3_block4_2_bn\n",
      "76 conv3_block4_2_relu\n",
      "77 conv3_block4_3_conv\n",
      "78 conv3_block4_3_bn\n",
      "79 conv3_block4_add\n",
      "80 conv3_block4_out\n",
      "81 conv4_block1_1_conv\n",
      "82 conv4_block1_1_bn\n",
      "83 conv4_block1_1_relu\n",
      "84 conv4_block1_2_conv\n",
      "85 conv4_block1_2_bn\n",
      "86 conv4_block1_2_relu\n",
      "87 conv4_block1_0_conv\n",
      "88 conv4_block1_3_conv\n",
      "89 conv4_block1_0_bn\n",
      "90 conv4_block1_3_bn\n",
      "91 conv4_block1_add\n",
      "92 conv4_block1_out\n",
      "93 conv4_block2_1_conv\n",
      "94 conv4_block2_1_bn\n",
      "95 conv4_block2_1_relu\n",
      "96 conv4_block2_2_conv\n",
      "97 conv4_block2_2_bn\n",
      "98 conv4_block2_2_relu\n",
      "99 conv4_block2_3_conv\n",
      "100 conv4_block2_3_bn\n",
      "101 conv4_block2_add\n",
      "102 conv4_block2_out\n",
      "103 conv4_block3_1_conv\n",
      "104 conv4_block3_1_bn\n",
      "105 conv4_block3_1_relu\n",
      "106 conv4_block3_2_conv\n",
      "107 conv4_block3_2_bn\n",
      "108 conv4_block3_2_relu\n",
      "109 conv4_block3_3_conv\n",
      "110 conv4_block3_3_bn\n",
      "111 conv4_block3_add\n",
      "112 conv4_block3_out\n",
      "113 conv4_block4_1_conv\n",
      "114 conv4_block4_1_bn\n",
      "115 conv4_block4_1_relu\n",
      "116 conv4_block4_2_conv\n",
      "117 conv4_block4_2_bn\n",
      "118 conv4_block4_2_relu\n",
      "119 conv4_block4_3_conv\n",
      "120 conv4_block4_3_bn\n",
      "121 conv4_block4_add\n",
      "122 conv4_block4_out\n",
      "123 conv4_block5_1_conv\n",
      "124 conv4_block5_1_bn\n",
      "125 conv4_block5_1_relu\n",
      "126 conv4_block5_2_conv\n",
      "127 conv4_block5_2_bn\n",
      "128 conv4_block5_2_relu\n",
      "129 conv4_block5_3_conv\n",
      "130 conv4_block5_3_bn\n",
      "131 conv4_block5_add\n",
      "132 conv4_block5_out\n",
      "133 conv4_block6_1_conv\n",
      "134 conv4_block6_1_bn\n",
      "135 conv4_block6_1_relu\n",
      "136 conv4_block6_2_conv\n",
      "137 conv4_block6_2_bn\n",
      "138 conv4_block6_2_relu\n",
      "139 conv4_block6_3_conv\n",
      "140 conv4_block6_3_bn\n",
      "141 conv4_block6_add\n",
      "142 conv4_block6_out\n",
      "143 conv5_block1_1_conv\n",
      "144 conv5_block1_1_bn\n",
      "145 conv5_block1_1_relu\n",
      "146 conv5_block1_2_conv\n",
      "147 conv5_block1_2_bn\n",
      "148 conv5_block1_2_relu\n",
      "149 conv5_block1_0_conv\n",
      "150 conv5_block1_3_conv\n",
      "151 conv5_block1_0_bn\n",
      "152 conv5_block1_3_bn\n",
      "153 conv5_block1_add\n",
      "154 conv5_block1_out\n",
      "155 conv5_block2_1_conv\n",
      "156 conv5_block2_1_bn\n",
      "157 conv5_block2_1_relu\n",
      "158 conv5_block2_2_conv\n",
      "159 conv5_block2_2_bn\n",
      "160 conv5_block2_2_relu\n",
      "161 conv5_block2_3_conv\n",
      "162 conv5_block2_3_bn\n",
      "163 conv5_block2_add\n",
      "164 conv5_block2_out\n",
      "165 conv5_block3_1_conv\n",
      "166 conv5_block3_1_bn\n",
      "167 conv5_block3_1_relu\n",
      "168 conv5_block3_2_conv\n",
      "169 conv5_block3_2_bn\n",
      "170 conv5_block3_2_relu\n",
      "171 conv5_block3_3_conv\n",
      "172 conv5_block3_3_bn\n",
      "173 conv5_block3_add\n",
      "174 conv5_block3_out\n",
      "175 global_average_pooling2d\n",
      "176 dense\n",
      "177 dropout\n",
      "178 dense_1\n"
     ]
    }
   ],
   "source": [
    "# lets check how many layers model has\n",
    "for i, layer in enumerate(resnet_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets unfreeze some layers\n",
    "for layer in resnet_model.layers[:143]:\n",
    "    layer.trainable = False\n",
    "for layer in resnet_model.layers[143:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1805/1805 [==============================] - 859s 474ms/step - loss: 7.1371 - accuracy: 0.0139 - val_loss: 7.0354 - val_accuracy: 0.0140\n",
      "Epoch 2/30\n",
      "1805/1805 [==============================] - 854s 473ms/step - loss: 7.0205 - accuracy: 0.0141 - val_loss: 6.9905 - val_accuracy: 0.0140\n",
      "Epoch 3/30\n",
      "1805/1805 [==============================] - 853s 472ms/step - loss: 6.9775 - accuracy: 0.0147 - val_loss: 7.0051 - val_accuracy: 0.0141\n",
      "Epoch 4/30\n",
      "1805/1805 [==============================] - 853s 472ms/step - loss: 6.9375 - accuracy: 0.0147 - val_loss: 6.8974 - val_accuracy: 0.0150\n",
      "Epoch 5/30\n",
      "1805/1805 [==============================] - 854s 473ms/step - loss: 6.8952 - accuracy: 0.0154 - val_loss: 6.8329 - val_accuracy: 0.0176\n",
      "Epoch 6/30\n",
      "1805/1805 [==============================] - 856s 474ms/step - loss: 6.8244 - accuracy: 0.0171 - val_loss: 6.8324 - val_accuracy: 0.0184\n",
      "Epoch 7/30\n",
      "1805/1805 [==============================] - 855s 473ms/step - loss: 6.7849 - accuracy: 0.0171 - val_loss: 6.7441 - val_accuracy: 0.0184\n",
      "Epoch 8/30\n",
      "1805/1805 [==============================] - 854s 473ms/step - loss: 6.7535 - accuracy: 0.0174 - val_loss: 6.7570 - val_accuracy: 0.0173\n",
      "Epoch 9/30\n",
      "1805/1805 [==============================] - 855s 474ms/step - loss: 6.7208 - accuracy: 0.0184 - val_loss: 6.7537 - val_accuracy: 0.0182\n",
      "Epoch 10/30\n",
      "1805/1805 [==============================] - 858s 475ms/step - loss: 6.6902 - accuracy: 0.0186 - val_loss: 6.6978 - val_accuracy: 0.0187\n",
      "Epoch 11/30\n",
      "1805/1805 [==============================] - 858s 476ms/step - loss: 6.6595 - accuracy: 0.0192 - val_loss: 7.0380 - val_accuracy: 0.0161\n",
      "Epoch 12/30\n",
      "1805/1805 [==============================] - 857s 475ms/step - loss: 6.6318 - accuracy: 0.0191 - val_loss: 6.6559 - val_accuracy: 0.0208\n",
      "Epoch 13/30\n",
      "1805/1805 [==============================] - 860s 476ms/step - loss: 6.6093 - accuracy: 0.0192 - val_loss: 6.5879 - val_accuracy: 0.0207\n",
      "Epoch 14/30\n",
      "1805/1805 [==============================] - 858s 475ms/step - loss: 6.5804 - accuracy: 0.0199 - val_loss: 6.6233 - val_accuracy: 0.0212\n",
      "Epoch 15/30\n",
      "1805/1805 [==============================] - 858s 476ms/step - loss: 6.5544 - accuracy: 0.0205 - val_loss: 6.5883 - val_accuracy: 0.0216\n",
      "Epoch 16/30\n",
      "1805/1805 [==============================] - 860s 477ms/step - loss: 6.5284 - accuracy: 0.0212 - val_loss: 6.5253 - val_accuracy: 0.0217\n",
      "Epoch 17/30\n",
      "1805/1805 [==============================] - 859s 476ms/step - loss: 6.4951 - accuracy: 0.0217 - val_loss: 6.6298 - val_accuracy: 0.0225\n",
      "Epoch 18/30\n",
      "1805/1805 [==============================] - 859s 476ms/step - loss: 6.4684 - accuracy: 0.0221 - val_loss: 6.4960 - val_accuracy: 0.0246\n",
      "Epoch 19/30\n",
      "1805/1805 [==============================] - 858s 476ms/step - loss: 6.4474 - accuracy: 0.0224 - val_loss: 6.4841 - val_accuracy: 0.0240\n",
      "Epoch 20/30\n",
      "1805/1805 [==============================] - 860s 476ms/step - loss: 6.4189 - accuracy: 0.0227 - val_loss: 6.7709 - val_accuracy: 0.0206\n",
      "Epoch 21/30\n",
      "1805/1805 [==============================] - 862s 477ms/step - loss: 6.3914 - accuracy: 0.0235 - val_loss: 6.5027 - val_accuracy: 0.0236\n",
      "Epoch 22/30\n",
      "1805/1805 [==============================] - 863s 478ms/step - loss: 6.3707 - accuracy: 0.0245 - val_loss: 6.4644 - val_accuracy: 0.0258\n",
      "Epoch 23/30\n",
      "1805/1805 [==============================] - 862s 477ms/step - loss: 6.3473 - accuracy: 0.0248 - val_loss: 7.1310 - val_accuracy: 0.0185\n",
      "Epoch 24/30\n",
      "1805/1805 [==============================] - 861s 477ms/step - loss: 6.3232 - accuracy: 0.0245 - val_loss: 6.4440 - val_accuracy: 0.0256\n",
      "Epoch 25/30\n",
      "1805/1805 [==============================] - 864s 478ms/step - loss: 6.3041 - accuracy: 0.0258 - val_loss: 7.0028 - val_accuracy: 0.0195\n",
      "Epoch 26/30\n",
      "1805/1805 [==============================] - 868s 481ms/step - loss: 6.2846 - accuracy: 0.0259 - val_loss: 6.4728 - val_accuracy: 0.0256\n",
      "Epoch 27/30\n",
      "1805/1805 [==============================] - 865s 479ms/step - loss: 6.2661 - accuracy: 0.0268 - val_loss: 6.3422 - val_accuracy: 0.0289\n",
      "Epoch 28/30\n",
      "1805/1805 [==============================] - 864s 478ms/step - loss: 6.2511 - accuracy: 0.0267 - val_loss: 6.5324 - val_accuracy: 0.0256\n",
      "Epoch 29/30\n",
      "1805/1805 [==============================] - 864s 479ms/step - loss: 6.2315 - accuracy: 0.0275 - val_loss: 7.4377 - val_accuracy: 0.0191\n",
      "Epoch 30/30\n",
      "1805/1805 [==============================] - 864s 479ms/step - loss: 6.2145 - accuracy: 0.0277 - val_loss: 6.4220 - val_accuracy: 0.0272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27d3a922610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets compile the model again and train it\n",
    "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_model.fit(train_data, validation_data=val_data, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/602 [==============================] - 160s 266ms/step - loss: 6.4663 - accuracy: 0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# lets evaluate the model and save it\n",
    "resnet_model.evaluate(test_data)\n",
    "saveModel(resnet_model, 'resnet_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on InceptionV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1805/1805 [==============================] - 536s 295ms/step - loss: 6.5079 - accuracy: 0.0300 - val_loss: 6.1377 - val_accuracy: 0.0407\n",
      "Epoch 2/10\n",
      "1805/1805 [==============================] - 502s 278ms/step - loss: 5.9192 - accuracy: 0.0467 - val_loss: 5.9074 - val_accuracy: 0.0468\n",
      "Epoch 3/10\n",
      "1805/1805 [==============================] - 501s 277ms/step - loss: 5.7388 - accuracy: 0.0547 - val_loss: 5.8419 - val_accuracy: 0.0520\n",
      "Epoch 4/10\n",
      "1805/1805 [==============================] - 489s 271ms/step - loss: 5.6420 - accuracy: 0.0594 - val_loss: 5.7502 - val_accuracy: 0.0567\n",
      "Epoch 5/10\n",
      "1805/1805 [==============================] - 487s 270ms/step - loss: 5.5804 - accuracy: 0.0629 - val_loss: 5.7331 - val_accuracy: 0.0601\n",
      "Epoch 6/10\n",
      "1805/1805 [==============================] - 483s 268ms/step - loss: 5.5447 - accuracy: 0.0654 - val_loss: 5.7264 - val_accuracy: 0.0586\n",
      "Epoch 7/10\n",
      "1805/1805 [==============================] - 482s 267ms/step - loss: 5.5236 - accuracy: 0.0662 - val_loss: 5.7092 - val_accuracy: 0.0590\n",
      "Epoch 8/10\n",
      "1805/1805 [==============================] - 484s 268ms/step - loss: 5.4986 - accuracy: 0.0674 - val_loss: 5.7062 - val_accuracy: 0.0585\n",
      "Epoch 9/10\n",
      "1805/1805 [==============================] - 487s 270ms/step - loss: 5.4947 - accuracy: 0.0675 - val_loss: 5.7597 - val_accuracy: 0.0649\n",
      "Epoch 10/10\n",
      "1805/1805 [==============================] - 482s 267ms/step - loss: 5.4772 - accuracy: 0.0698 - val_loss: 5.7163 - val_accuracy: 0.0635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27d9f280890>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1727, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 conv2d\n",
      "2 batch_normalization\n",
      "3 activation\n",
      "4 conv2d_1\n",
      "5 batch_normalization_1\n",
      "6 activation_1\n",
      "7 conv2d_2\n",
      "8 batch_normalization_2\n",
      "9 activation_2\n",
      "10 max_pooling2d\n",
      "11 conv2d_3\n",
      "12 batch_normalization_3\n",
      "13 activation_3\n",
      "14 conv2d_4\n",
      "15 batch_normalization_4\n",
      "16 activation_4\n",
      "17 max_pooling2d_1\n",
      "18 conv2d_8\n",
      "19 batch_normalization_8\n",
      "20 activation_8\n",
      "21 conv2d_6\n",
      "22 conv2d_9\n",
      "23 batch_normalization_6\n",
      "24 batch_normalization_9\n",
      "25 activation_6\n",
      "26 activation_9\n",
      "27 average_pooling2d\n",
      "28 conv2d_5\n",
      "29 conv2d_7\n",
      "30 conv2d_10\n",
      "31 conv2d_11\n",
      "32 batch_normalization_5\n",
      "33 batch_normalization_7\n",
      "34 batch_normalization_10\n",
      "35 batch_normalization_11\n",
      "36 activation_5\n",
      "37 activation_7\n",
      "38 activation_10\n",
      "39 activation_11\n",
      "40 mixed0\n",
      "41 conv2d_15\n",
      "42 batch_normalization_15\n",
      "43 activation_15\n",
      "44 conv2d_13\n",
      "45 conv2d_16\n",
      "46 batch_normalization_13\n",
      "47 batch_normalization_16\n",
      "48 activation_13\n",
      "49 activation_16\n",
      "50 average_pooling2d_1\n",
      "51 conv2d_12\n",
      "52 conv2d_14\n",
      "53 conv2d_17\n",
      "54 conv2d_18\n",
      "55 batch_normalization_12\n",
      "56 batch_normalization_14\n",
      "57 batch_normalization_17\n",
      "58 batch_normalization_18\n",
      "59 activation_12\n",
      "60 activation_14\n",
      "61 activation_17\n",
      "62 activation_18\n",
      "63 mixed1\n",
      "64 conv2d_22\n",
      "65 batch_normalization_22\n",
      "66 activation_22\n",
      "67 conv2d_20\n",
      "68 conv2d_23\n",
      "69 batch_normalization_20\n",
      "70 batch_normalization_23\n",
      "71 activation_20\n",
      "72 activation_23\n",
      "73 average_pooling2d_2\n",
      "74 conv2d_19\n",
      "75 conv2d_21\n",
      "76 conv2d_24\n",
      "77 conv2d_25\n",
      "78 batch_normalization_19\n",
      "79 batch_normalization_21\n",
      "80 batch_normalization_24\n",
      "81 batch_normalization_25\n",
      "82 activation_19\n",
      "83 activation_21\n",
      "84 activation_24\n",
      "85 activation_25\n",
      "86 mixed2\n",
      "87 conv2d_27\n",
      "88 batch_normalization_27\n",
      "89 activation_27\n",
      "90 conv2d_28\n",
      "91 batch_normalization_28\n",
      "92 activation_28\n",
      "93 conv2d_26\n",
      "94 conv2d_29\n",
      "95 batch_normalization_26\n",
      "96 batch_normalization_29\n",
      "97 activation_26\n",
      "98 activation_29\n",
      "99 max_pooling2d_2\n",
      "100 mixed3\n",
      "101 conv2d_34\n",
      "102 batch_normalization_34\n",
      "103 activation_34\n",
      "104 conv2d_35\n",
      "105 batch_normalization_35\n",
      "106 activation_35\n",
      "107 conv2d_31\n",
      "108 conv2d_36\n",
      "109 batch_normalization_31\n",
      "110 batch_normalization_36\n",
      "111 activation_31\n",
      "112 activation_36\n",
      "113 conv2d_32\n",
      "114 conv2d_37\n",
      "115 batch_normalization_32\n",
      "116 batch_normalization_37\n",
      "117 activation_32\n",
      "118 activation_37\n",
      "119 average_pooling2d_3\n",
      "120 conv2d_30\n",
      "121 conv2d_33\n",
      "122 conv2d_38\n",
      "123 conv2d_39\n",
      "124 batch_normalization_30\n",
      "125 batch_normalization_33\n",
      "126 batch_normalization_38\n",
      "127 batch_normalization_39\n",
      "128 activation_30\n",
      "129 activation_33\n",
      "130 activation_38\n",
      "131 activation_39\n",
      "132 mixed4\n",
      "133 conv2d_44\n",
      "134 batch_normalization_44\n",
      "135 activation_44\n",
      "136 conv2d_45\n",
      "137 batch_normalization_45\n",
      "138 activation_45\n",
      "139 conv2d_41\n",
      "140 conv2d_46\n",
      "141 batch_normalization_41\n",
      "142 batch_normalization_46\n",
      "143 activation_41\n",
      "144 activation_46\n",
      "145 conv2d_42\n",
      "146 conv2d_47\n",
      "147 batch_normalization_42\n",
      "148 batch_normalization_47\n",
      "149 activation_42\n",
      "150 activation_47\n",
      "151 average_pooling2d_4\n",
      "152 conv2d_40\n",
      "153 conv2d_43\n",
      "154 conv2d_48\n",
      "155 conv2d_49\n",
      "156 batch_normalization_40\n",
      "157 batch_normalization_43\n",
      "158 batch_normalization_48\n",
      "159 batch_normalization_49\n",
      "160 activation_40\n",
      "161 activation_43\n",
      "162 activation_48\n",
      "163 activation_49\n",
      "164 mixed5\n",
      "165 conv2d_54\n",
      "166 batch_normalization_54\n",
      "167 activation_54\n",
      "168 conv2d_55\n",
      "169 batch_normalization_55\n",
      "170 activation_55\n",
      "171 conv2d_51\n",
      "172 conv2d_56\n",
      "173 batch_normalization_51\n",
      "174 batch_normalization_56\n",
      "175 activation_51\n",
      "176 activation_56\n",
      "177 conv2d_52\n",
      "178 conv2d_57\n",
      "179 batch_normalization_52\n",
      "180 batch_normalization_57\n",
      "181 activation_52\n",
      "182 activation_57\n",
      "183 average_pooling2d_5\n",
      "184 conv2d_50\n",
      "185 conv2d_53\n",
      "186 conv2d_58\n",
      "187 conv2d_59\n",
      "188 batch_normalization_50\n",
      "189 batch_normalization_53\n",
      "190 batch_normalization_58\n",
      "191 batch_normalization_59\n",
      "192 activation_50\n",
      "193 activation_53\n",
      "194 activation_58\n",
      "195 activation_59\n",
      "196 mixed6\n",
      "197 conv2d_64\n",
      "198 batch_normalization_64\n",
      "199 activation_64\n",
      "200 conv2d_65\n",
      "201 batch_normalization_65\n",
      "202 activation_65\n",
      "203 conv2d_61\n",
      "204 conv2d_66\n",
      "205 batch_normalization_61\n",
      "206 batch_normalization_66\n",
      "207 activation_61\n",
      "208 activation_66\n",
      "209 conv2d_62\n",
      "210 conv2d_67\n",
      "211 batch_normalization_62\n",
      "212 batch_normalization_67\n",
      "213 activation_62\n",
      "214 activation_67\n",
      "215 average_pooling2d_6\n",
      "216 conv2d_60\n",
      "217 conv2d_63\n",
      "218 conv2d_68\n",
      "219 conv2d_69\n",
      "220 batch_normalization_60\n",
      "221 batch_normalization_63\n",
      "222 batch_normalization_68\n",
      "223 batch_normalization_69\n",
      "224 activation_60\n",
      "225 activation_63\n",
      "226 activation_68\n",
      "227 activation_69\n",
      "228 mixed7\n",
      "229 conv2d_72\n",
      "230 batch_normalization_72\n",
      "231 activation_72\n",
      "232 conv2d_73\n",
      "233 batch_normalization_73\n",
      "234 activation_73\n",
      "235 conv2d_70\n",
      "236 conv2d_74\n",
      "237 batch_normalization_70\n",
      "238 batch_normalization_74\n",
      "239 activation_70\n",
      "240 activation_74\n",
      "241 conv2d_71\n",
      "242 conv2d_75\n",
      "243 batch_normalization_71\n",
      "244 batch_normalization_75\n",
      "245 activation_71\n",
      "246 activation_75\n",
      "247 max_pooling2d_3\n",
      "248 mixed8\n",
      "249 conv2d_80\n",
      "250 batch_normalization_80\n",
      "251 activation_80\n",
      "252 conv2d_77\n",
      "253 conv2d_81\n",
      "254 batch_normalization_77\n",
      "255 batch_normalization_81\n",
      "256 activation_77\n",
      "257 activation_81\n",
      "258 conv2d_78\n",
      "259 conv2d_79\n",
      "260 conv2d_82\n",
      "261 conv2d_83\n",
      "262 average_pooling2d_7\n",
      "263 conv2d_76\n",
      "264 batch_normalization_78\n",
      "265 batch_normalization_79\n",
      "266 batch_normalization_82\n",
      "267 batch_normalization_83\n",
      "268 conv2d_84\n",
      "269 batch_normalization_76\n",
      "270 activation_78\n",
      "271 activation_79\n",
      "272 activation_82\n",
      "273 activation_83\n",
      "274 batch_normalization_84\n",
      "275 activation_76\n",
      "276 mixed9_0\n",
      "277 concatenate\n",
      "278 activation_84\n",
      "279 mixed9\n",
      "280 conv2d_89\n",
      "281 batch_normalization_89\n",
      "282 activation_89\n",
      "283 conv2d_86\n",
      "284 conv2d_90\n",
      "285 batch_normalization_86\n",
      "286 batch_normalization_90\n",
      "287 activation_86\n",
      "288 activation_90\n",
      "289 conv2d_87\n",
      "290 conv2d_88\n",
      "291 conv2d_91\n",
      "292 conv2d_92\n",
      "293 average_pooling2d_8\n",
      "294 conv2d_85\n",
      "295 batch_normalization_87\n",
      "296 batch_normalization_88\n",
      "297 batch_normalization_91\n",
      "298 batch_normalization_92\n",
      "299 conv2d_93\n",
      "300 batch_normalization_85\n",
      "301 activation_87\n",
      "302 activation_88\n",
      "303 activation_91\n",
      "304 activation_92\n",
      "305 batch_normalization_93\n",
      "306 activation_85\n",
      "307 mixed9_1\n",
      "308 concatenate_1\n",
      "309 activation_93\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1805/1805 [==============================] - 590s 325ms/step - loss: 5.2877 - accuracy: 0.0779 - val_loss: 5.1379 - val_accuracy: 0.0984\n",
      "Epoch 2/30\n",
      "1805/1805 [==============================] - 585s 324ms/step - loss: 4.8073 - accuracy: 0.1184 - val_loss: 5.0015 - val_accuracy: 0.1152\n",
      "Epoch 3/30\n",
      "1805/1805 [==============================] - 585s 324ms/step - loss: 4.5421 - accuracy: 0.1458 - val_loss: 4.8162 - val_accuracy: 0.1347\n",
      "Epoch 4/30\n",
      "1805/1805 [==============================] - 589s 326ms/step - loss: 4.3195 - accuracy: 0.1711 - val_loss: 4.7508 - val_accuracy: 0.1416\n",
      "Epoch 5/30\n",
      "1805/1805 [==============================] - 588s 326ms/step - loss: 4.1139 - accuracy: 0.1958 - val_loss: 4.7058 - val_accuracy: 0.1513\n",
      "Epoch 6/30\n",
      "1805/1805 [==============================] - 589s 326ms/step - loss: 3.9242 - accuracy: 0.2189 - val_loss: 4.6601 - val_accuracy: 0.1619\n",
      "Epoch 7/30\n",
      "1805/1805 [==============================] - 587s 325ms/step - loss: 3.7418 - accuracy: 0.2448 - val_loss: 4.6490 - val_accuracy: 0.1714\n",
      "Epoch 8/30\n",
      "1805/1805 [==============================] - 585s 324ms/step - loss: 3.5735 - accuracy: 0.2681 - val_loss: 4.6674 - val_accuracy: 0.1758\n",
      "Epoch 9/30\n",
      "1805/1805 [==============================] - 586s 325ms/step - loss: 3.4155 - accuracy: 0.2891 - val_loss: 4.6991 - val_accuracy: 0.1823\n",
      "Epoch 10/30\n",
      "1805/1805 [==============================] - 586s 324ms/step - loss: 3.2578 - accuracy: 0.3144 - val_loss: 4.7007 - val_accuracy: 0.1838\n",
      "Epoch 11/30\n",
      "1805/1805 [==============================] - 588s 326ms/step - loss: 3.1074 - accuracy: 0.3349 - val_loss: 4.7382 - val_accuracy: 0.1920\n",
      "Epoch 12/30\n",
      "1805/1805 [==============================] - 588s 326ms/step - loss: 2.9657 - accuracy: 0.3572 - val_loss: 4.8239 - val_accuracy: 0.1937\n",
      "Epoch 13/30\n",
      "1805/1805 [==============================] - 588s 326ms/step - loss: 2.8295 - accuracy: 0.3800 - val_loss: 4.8073 - val_accuracy: 0.1966\n",
      "Epoch 14/30\n",
      "1805/1805 [==============================] - 588s 326ms/step - loss: 2.6932 - accuracy: 0.4013 - val_loss: 4.8591 - val_accuracy: 0.2022\n",
      "Epoch 15/30\n",
      "1805/1805 [==============================] - 587s 325ms/step - loss: 2.5735 - accuracy: 0.4209 - val_loss: 4.8813 - val_accuracy: 0.2012\n",
      "Epoch 16/30\n",
      "1805/1805 [==============================] - 587s 325ms/step - loss: 2.4468 - accuracy: 0.4450 - val_loss: 4.9564 - val_accuracy: 0.2034\n",
      "Epoch 17/30\n",
      "1805/1805 [==============================] - 590s 327ms/step - loss: 2.3453 - accuracy: 0.4616 - val_loss: 5.0274 - val_accuracy: 0.2085\n",
      "Epoch 18/30\n",
      "1805/1805 [==============================] - 589s 326ms/step - loss: 2.2346 - accuracy: 0.4812 - val_loss: 5.0146 - val_accuracy: 0.2152\n",
      "Epoch 19/30\n",
      "1805/1805 [==============================] - 591s 327ms/step - loss: 2.1196 - accuracy: 0.5035 - val_loss: 5.1794 - val_accuracy: 0.2137\n",
      "Epoch 20/30\n",
      "1805/1805 [==============================] - 589s 326ms/step - loss: 2.0223 - accuracy: 0.5221 - val_loss: 5.1273 - val_accuracy: 0.2151\n",
      "Epoch 21/30\n",
      "1805/1805 [==============================] - 590s 327ms/step - loss: 1.9251 - accuracy: 0.5420 - val_loss: 5.2864 - val_accuracy: 0.2151\n",
      "Epoch 22/30\n",
      "1805/1805 [==============================] - 588s 326ms/step - loss: 1.8424 - accuracy: 0.5572 - val_loss: 5.3608 - val_accuracy: 0.2149\n",
      "Epoch 23/30\n",
      "1805/1805 [==============================] - 587s 325ms/step - loss: 1.7612 - accuracy: 0.5731 - val_loss: 5.3925 - val_accuracy: 0.2185\n",
      "Epoch 24/30\n",
      "1805/1805 [==============================] - 590s 327ms/step - loss: 1.6830 - accuracy: 0.5898 - val_loss: 5.4939 - val_accuracy: 0.2191\n",
      "Epoch 25/30\n",
      "1805/1805 [==============================] - 603s 334ms/step - loss: 1.6215 - accuracy: 0.6034 - val_loss: 5.5411 - val_accuracy: 0.2252\n",
      "Epoch 26/30\n",
      "1805/1805 [==============================] - 600s 332ms/step - loss: 1.5322 - accuracy: 0.6199 - val_loss: 5.6785 - val_accuracy: 0.2215\n",
      "Epoch 27/30\n",
      "1805/1805 [==============================] - 598s 331ms/step - loss: 1.4755 - accuracy: 0.6322 - val_loss: 5.7316 - val_accuracy: 0.2246\n",
      "Epoch 28/30\n",
      "1805/1805 [==============================] - 596s 330ms/step - loss: 1.4112 - accuracy: 0.6481 - val_loss: 5.7645 - val_accuracy: 0.2265\n",
      "Epoch 29/30\n",
      "1805/1805 [==============================] - 598s 331ms/step - loss: 1.3463 - accuracy: 0.6621 - val_loss: 5.7974 - val_accuracy: 0.2271\n",
      "Epoch 30/30\n",
      "1805/1805 [==============================] - 599s 332ms/step - loss: 1.2861 - accuracy: 0.6750 - val_loss: 5.8946 - val_accuracy: 0.2240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27da60b9bd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=val_data, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/602 [==============================] - 79s 130ms/step - loss: 5.8689 - accuracy: 0.2356\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "model.evaluate(test_data)\n",
    "#Save the model\n",
    "saveModel(model, 'inception_v3_mushrooms')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
