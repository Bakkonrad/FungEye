{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from PIL import Image\n",
    "\n",
    "MUSHROOMS_PATH = 'mushrooms_dataset'\n",
    "\n",
    "# Directory for the images and its subdirectories\n",
    "images_dir = os.path.join(MUSHROOMS_PATH, 'images')\n",
    "subdirs = [os.path.join(images_dir, subdir) for subdir in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, subdir))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have some ideas for dividing the dataset into training and testing sets. We can use the train_test_split function from scikit-learn to divide the dataset into training and testing sets.\n",
    "#But for that we will have to put the every image into array and then into a dataframe\n",
    "#Then we will have to use ImageDataGenerator and flow_from_dataframe to load the images from the dataframe\n",
    "\n",
    "#Second idea is to manually create the test set by taking 20% of the images from each class and putting them into a separate directory.\n",
    "#We will then use ImageDataGenerator and flow_from_directory to load the images from the directory.\n",
    "\n",
    "#In both ideas we need to take in consider stratification, so that the distribution of classes in the training and testing sets is similar.\n",
    "#For example, if in one class there are 10 images and in another one there are 8 images, we want both  of them to have the same percentage of images in the training and testing sets.\n",
    "\n",
    "#Third idea is to use the splitfolders library to divide the dataset into training and testing sets.\n",
    "#But again we have to stratify the dataset which is not supported by that library.\n",
    "\n",
    "#So the first idea might require a lot of memory usage, the second idea needs us to well do this manually which is not very efficient.\n",
    "#And the third idea is not supporting stratification.\n",
    "\n",
    "#So for now we will use the first idea and divide the dataset into training and testing sets using the train_test_split function from scikit-learn which has the stratify parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So the process with the first idea is as follows:\n",
    "#1. Load the images and its corresponding labels into a dataframe.\n",
    "#2. Divide the dataset into training and testing sets using the train_test_split function from scikit-learn with stratification.\n",
    "#3. Use ImageDataGenerator and flow_from_dataframe to load the images from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for subdir in subdirs:\n",
    "    label = os.path.basename(subdir) #we specify the label for each image\n",
    "    for filename in os.listdir(subdir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            data.append((os.path.join(subdir, filename), label)) #we need to include whole path of the image for using flow_from_dataframe because it reads the images directly from the file system using the paths provided in the DataFrame.\n",
    "data_df = pd.DataFrame(data, columns=['filename', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mushrooms_dataset\\images\\Abortiporus_biennis\\1...</td>\n",
       "      <td>Abortiporus_biennis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename                label\n",
       "0  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis\n",
       "1  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis\n",
       "2  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis\n",
       "3  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis\n",
       "4  mushrooms_dataset\\images\\Abortiporus_biennis\\1...  Abortiporus_biennis"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_df, test_size=0.2, stratify=data_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120015 validated image filenames belonging to 3976 classes.\n",
      "Found 40005 validated image filenames belonging to 3976 classes.\n",
      "Found 40005 validated image filenames belonging to 3976 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.2, shear_range=0.2, validation_split=0.25,) #we use 25% from the 80% of the training set as the validation set which will be the same amount as the testing set\n",
    "\n",
    "train_data = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(128, 128),\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "\n",
    "val_data = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(128, 128),\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_data = datagen_test.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(128, 128),\n",
    "    class_mode='categorical',\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---DONE---\n",
    "#It worked, but why do we have only 6903 classes in the test set and 7504 in training and validation sets? \n",
    "# Perhaps there are not enough images in some classes???\n",
    "\n",
    "# It is probably true beacause when we use stratify parameter in train_test_split function, it tries to keep the distribution of classes in the training and testing sets similar.\n",
    "# But if there are not enough images in some classes, it will not be able to keep the distribution of classes similar in the training and testing sets.\n",
    "# So we have few solutions to this\n",
    "# 1. Ensure that each class has a minimum number of instances before splitting the data into training and testing sets - that worked!!!!\n",
    "# 2. Use the stratify sampling only on the classes with sufficient instances, and randomly split the ones with too few instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could have - we could create our own model but since we have massive amount of images its easier to use pre-trained one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the first model\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(7504, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_data, validation_data=val_data, epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pr√≥ba treningu na pretrenowanym modelu InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1876/1876 [==============================] - 744s 395ms/step - loss: 7.2602 - accuracy: 0.0244 - val_loss: 6.9447 - val_accuracy: 0.0301\n",
      "Epoch 2/10\n",
      "1876/1876 [==============================] - 530s 282ms/step - loss: 6.7019 - accuracy: 0.0348 - val_loss: 6.7207 - val_accuracy: 0.0339\n",
      "Epoch 3/10\n",
      "1876/1876 [==============================] - 530s 283ms/step - loss: 6.4736 - accuracy: 0.0381 - val_loss: 6.6073 - val_accuracy: 0.0386\n",
      "Epoch 4/10\n",
      "1876/1876 [==============================] - 526s 280ms/step - loss: 6.3484 - accuracy: 0.0407 - val_loss: 6.5646 - val_accuracy: 0.0412\n",
      "Epoch 5/10\n",
      "1876/1876 [==============================] - 527s 281ms/step - loss: 6.2764 - accuracy: 0.0425 - val_loss: 6.5708 - val_accuracy: 0.0408\n",
      "Epoch 6/10\n",
      "1876/1876 [==============================] - 746s 398ms/step - loss: 6.2299 - accuracy: 0.0442 - val_loss: 6.5625 - val_accuracy: 0.0410\n",
      "Epoch 7/10\n",
      "1876/1876 [==============================] - 1231s 656ms/step - loss: 6.2064 - accuracy: 0.0452 - val_loss: 6.5824 - val_accuracy: 0.0426\n",
      "Epoch 8/10\n",
      "1876/1876 [==============================] - 1277s 681ms/step - loss: 6.1833 - accuracy: 0.0458 - val_loss: 6.5759 - val_accuracy: 0.0448\n",
      "Epoch 9/10\n",
      "1876/1876 [==============================] - 1253s 668ms/step - loss: 6.1606 - accuracy: 0.0457 - val_loss: 6.5627 - val_accuracy: 0.0408\n",
      "Epoch 10/10\n",
      "1876/1876 [==============================] - 1304s 695ms/step - loss: 6.1557 - accuracy: 0.0467 - val_loss: 6.6677 - val_accuracy: 0.0406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x211298bef90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(3976, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv2d\n",
      "2 batch_normalization\n",
      "3 activation\n",
      "4 conv2d_1\n",
      "5 batch_normalization_1\n",
      "6 activation_1\n",
      "7 conv2d_2\n",
      "8 batch_normalization_2\n",
      "9 activation_2\n",
      "10 max_pooling2d\n",
      "11 conv2d_3\n",
      "12 batch_normalization_3\n",
      "13 activation_3\n",
      "14 conv2d_4\n",
      "15 batch_normalization_4\n",
      "16 activation_4\n",
      "17 max_pooling2d_1\n",
      "18 conv2d_8\n",
      "19 batch_normalization_8\n",
      "20 activation_8\n",
      "21 conv2d_6\n",
      "22 conv2d_9\n",
      "23 batch_normalization_6\n",
      "24 batch_normalization_9\n",
      "25 activation_6\n",
      "26 activation_9\n",
      "27 average_pooling2d\n",
      "28 conv2d_5\n",
      "29 conv2d_7\n",
      "30 conv2d_10\n",
      "31 conv2d_11\n",
      "32 batch_normalization_5\n",
      "33 batch_normalization_7\n",
      "34 batch_normalization_10\n",
      "35 batch_normalization_11\n",
      "36 activation_5\n",
      "37 activation_7\n",
      "38 activation_10\n",
      "39 activation_11\n",
      "40 mixed0\n",
      "41 conv2d_15\n",
      "42 batch_normalization_15\n",
      "43 activation_15\n",
      "44 conv2d_13\n",
      "45 conv2d_16\n",
      "46 batch_normalization_13\n",
      "47 batch_normalization_16\n",
      "48 activation_13\n",
      "49 activation_16\n",
      "50 average_pooling2d_1\n",
      "51 conv2d_12\n",
      "52 conv2d_14\n",
      "53 conv2d_17\n",
      "54 conv2d_18\n",
      "55 batch_normalization_12\n",
      "56 batch_normalization_14\n",
      "57 batch_normalization_17\n",
      "58 batch_normalization_18\n",
      "59 activation_12\n",
      "60 activation_14\n",
      "61 activation_17\n",
      "62 activation_18\n",
      "63 mixed1\n",
      "64 conv2d_22\n",
      "65 batch_normalization_22\n",
      "66 activation_22\n",
      "67 conv2d_20\n",
      "68 conv2d_23\n",
      "69 batch_normalization_20\n",
      "70 batch_normalization_23\n",
      "71 activation_20\n",
      "72 activation_23\n",
      "73 average_pooling2d_2\n",
      "74 conv2d_19\n",
      "75 conv2d_21\n",
      "76 conv2d_24\n",
      "77 conv2d_25\n",
      "78 batch_normalization_19\n",
      "79 batch_normalization_21\n",
      "80 batch_normalization_24\n",
      "81 batch_normalization_25\n",
      "82 activation_19\n",
      "83 activation_21\n",
      "84 activation_24\n",
      "85 activation_25\n",
      "86 mixed2\n",
      "87 conv2d_27\n",
      "88 batch_normalization_27\n",
      "89 activation_27\n",
      "90 conv2d_28\n",
      "91 batch_normalization_28\n",
      "92 activation_28\n",
      "93 conv2d_26\n",
      "94 conv2d_29\n",
      "95 batch_normalization_26\n",
      "96 batch_normalization_29\n",
      "97 activation_26\n",
      "98 activation_29\n",
      "99 max_pooling2d_2\n",
      "100 mixed3\n",
      "101 conv2d_34\n",
      "102 batch_normalization_34\n",
      "103 activation_34\n",
      "104 conv2d_35\n",
      "105 batch_normalization_35\n",
      "106 activation_35\n",
      "107 conv2d_31\n",
      "108 conv2d_36\n",
      "109 batch_normalization_31\n",
      "110 batch_normalization_36\n",
      "111 activation_31\n",
      "112 activation_36\n",
      "113 conv2d_32\n",
      "114 conv2d_37\n",
      "115 batch_normalization_32\n",
      "116 batch_normalization_37\n",
      "117 activation_32\n",
      "118 activation_37\n",
      "119 average_pooling2d_3\n",
      "120 conv2d_30\n",
      "121 conv2d_33\n",
      "122 conv2d_38\n",
      "123 conv2d_39\n",
      "124 batch_normalization_30\n",
      "125 batch_normalization_33\n",
      "126 batch_normalization_38\n",
      "127 batch_normalization_39\n",
      "128 activation_30\n",
      "129 activation_33\n",
      "130 activation_38\n",
      "131 activation_39\n",
      "132 mixed4\n",
      "133 conv2d_44\n",
      "134 batch_normalization_44\n",
      "135 activation_44\n",
      "136 conv2d_45\n",
      "137 batch_normalization_45\n",
      "138 activation_45\n",
      "139 conv2d_41\n",
      "140 conv2d_46\n",
      "141 batch_normalization_41\n",
      "142 batch_normalization_46\n",
      "143 activation_41\n",
      "144 activation_46\n",
      "145 conv2d_42\n",
      "146 conv2d_47\n",
      "147 batch_normalization_42\n",
      "148 batch_normalization_47\n",
      "149 activation_42\n",
      "150 activation_47\n",
      "151 average_pooling2d_4\n",
      "152 conv2d_40\n",
      "153 conv2d_43\n",
      "154 conv2d_48\n",
      "155 conv2d_49\n",
      "156 batch_normalization_40\n",
      "157 batch_normalization_43\n",
      "158 batch_normalization_48\n",
      "159 batch_normalization_49\n",
      "160 activation_40\n",
      "161 activation_43\n",
      "162 activation_48\n",
      "163 activation_49\n",
      "164 mixed5\n",
      "165 conv2d_54\n",
      "166 batch_normalization_54\n",
      "167 activation_54\n",
      "168 conv2d_55\n",
      "169 batch_normalization_55\n",
      "170 activation_55\n",
      "171 conv2d_51\n",
      "172 conv2d_56\n",
      "173 batch_normalization_51\n",
      "174 batch_normalization_56\n",
      "175 activation_51\n",
      "176 activation_56\n",
      "177 conv2d_52\n",
      "178 conv2d_57\n",
      "179 batch_normalization_52\n",
      "180 batch_normalization_57\n",
      "181 activation_52\n",
      "182 activation_57\n",
      "183 average_pooling2d_5\n",
      "184 conv2d_50\n",
      "185 conv2d_53\n",
      "186 conv2d_58\n",
      "187 conv2d_59\n",
      "188 batch_normalization_50\n",
      "189 batch_normalization_53\n",
      "190 batch_normalization_58\n",
      "191 batch_normalization_59\n",
      "192 activation_50\n",
      "193 activation_53\n",
      "194 activation_58\n",
      "195 activation_59\n",
      "196 mixed6\n",
      "197 conv2d_64\n",
      "198 batch_normalization_64\n",
      "199 activation_64\n",
      "200 conv2d_65\n",
      "201 batch_normalization_65\n",
      "202 activation_65\n",
      "203 conv2d_61\n",
      "204 conv2d_66\n",
      "205 batch_normalization_61\n",
      "206 batch_normalization_66\n",
      "207 activation_61\n",
      "208 activation_66\n",
      "209 conv2d_62\n",
      "210 conv2d_67\n",
      "211 batch_normalization_62\n",
      "212 batch_normalization_67\n",
      "213 activation_62\n",
      "214 activation_67\n",
      "215 average_pooling2d_6\n",
      "216 conv2d_60\n",
      "217 conv2d_63\n",
      "218 conv2d_68\n",
      "219 conv2d_69\n",
      "220 batch_normalization_60\n",
      "221 batch_normalization_63\n",
      "222 batch_normalization_68\n",
      "223 batch_normalization_69\n",
      "224 activation_60\n",
      "225 activation_63\n",
      "226 activation_68\n",
      "227 activation_69\n",
      "228 mixed7\n",
      "229 conv2d_72\n",
      "230 batch_normalization_72\n",
      "231 activation_72\n",
      "232 conv2d_73\n",
      "233 batch_normalization_73\n",
      "234 activation_73\n",
      "235 conv2d_70\n",
      "236 conv2d_74\n",
      "237 batch_normalization_70\n",
      "238 batch_normalization_74\n",
      "239 activation_70\n",
      "240 activation_74\n",
      "241 conv2d_71\n",
      "242 conv2d_75\n",
      "243 batch_normalization_71\n",
      "244 batch_normalization_75\n",
      "245 activation_71\n",
      "246 activation_75\n",
      "247 max_pooling2d_3\n",
      "248 mixed8\n",
      "249 conv2d_80\n",
      "250 batch_normalization_80\n",
      "251 activation_80\n",
      "252 conv2d_77\n",
      "253 conv2d_81\n",
      "254 batch_normalization_77\n",
      "255 batch_normalization_81\n",
      "256 activation_77\n",
      "257 activation_81\n",
      "258 conv2d_78\n",
      "259 conv2d_79\n",
      "260 conv2d_82\n",
      "261 conv2d_83\n",
      "262 average_pooling2d_7\n",
      "263 conv2d_76\n",
      "264 batch_normalization_78\n",
      "265 batch_normalization_79\n",
      "266 batch_normalization_82\n",
      "267 batch_normalization_83\n",
      "268 conv2d_84\n",
      "269 batch_normalization_76\n",
      "270 activation_78\n",
      "271 activation_79\n",
      "272 activation_82\n",
      "273 activation_83\n",
      "274 batch_normalization_84\n",
      "275 activation_76\n",
      "276 mixed9_0\n",
      "277 concatenate\n",
      "278 activation_84\n",
      "279 mixed9\n",
      "280 conv2d_89\n",
      "281 batch_normalization_89\n",
      "282 activation_89\n",
      "283 conv2d_86\n",
      "284 conv2d_90\n",
      "285 batch_normalization_86\n",
      "286 batch_normalization_90\n",
      "287 activation_86\n",
      "288 activation_90\n",
      "289 conv2d_87\n",
      "290 conv2d_88\n",
      "291 conv2d_91\n",
      "292 conv2d_92\n",
      "293 average_pooling2d_8\n",
      "294 conv2d_85\n",
      "295 batch_normalization_87\n",
      "296 batch_normalization_88\n",
      "297 batch_normalization_91\n",
      "298 batch_normalization_92\n",
      "299 conv2d_93\n",
      "300 batch_normalization_85\n",
      "301 activation_87\n",
      "302 activation_88\n",
      "303 activation_91\n",
      "304 activation_92\n",
      "305 batch_normalization_93\n",
      "306 activation_85\n",
      "307 mixed9_1\n",
      "308 concatenate_1\n",
      "309 activation_93\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1876/1876 [==============================] - 1502s 800ms/step - loss: 6.0247 - accuracy: 0.0504 - val_loss: 6.1153 - val_accuracy: 0.0603\n",
      "Epoch 2/10\n",
      "1876/1876 [==============================] - 1475s 786ms/step - loss: 5.6559 - accuracy: 0.0713 - val_loss: 6.0000 - val_accuracy: 0.0693\n",
      "Epoch 3/10\n",
      "1876/1876 [==============================] - 1523s 812ms/step - loss: 5.4556 - accuracy: 0.0854 - val_loss: 5.8874 - val_accuracy: 0.0807\n",
      "Epoch 4/10\n",
      "1876/1876 [==============================] - 1465s 781ms/step - loss: 5.2901 - accuracy: 0.0985 - val_loss: 5.8149 - val_accuracy: 0.0892\n",
      "Epoch 5/10\n",
      "1876/1876 [==============================] - 1348s 718ms/step - loss: 5.1454 - accuracy: 0.1115 - val_loss: 5.7604 - val_accuracy: 0.1006\n",
      "Epoch 6/10\n",
      "1876/1876 [==============================] - 669s 357ms/step - loss: 4.9925 - accuracy: 0.1244 - val_loss: 5.7603 - val_accuracy: 0.1075\n",
      "Epoch 7/10\n",
      "1876/1876 [==============================] - 653s 348ms/step - loss: 4.8548 - accuracy: 0.1382 - val_loss: 5.6614 - val_accuracy: 0.1125\n",
      "Epoch 8/10\n",
      "1876/1876 [==============================] - 668s 356ms/step - loss: 4.7135 - accuracy: 0.1511 - val_loss: 5.6877 - val_accuracy: 0.1164\n",
      "Epoch 9/10\n",
      "1876/1876 [==============================] - 643s 343ms/step - loss: 4.5619 - accuracy: 0.1682 - val_loss: 5.5599 - val_accuracy: 0.1264\n",
      "Epoch 10/10\n",
      "1876/1876 [==============================] - 643s 343ms/step - loss: 4.4182 - accuracy: 0.1855 - val_loss: 5.6123 - val_accuracy: 0.1377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2113176db90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626/626 [==============================] - 133s 212ms/step - loss: 5.4644 - accuracy: 0.1455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.464415073394775, 0.1455318033695221]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "model.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
